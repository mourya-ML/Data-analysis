# -*- coding: utf-8 -*-
"""NLP_Sentiment Analysis 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FmItBvHx3FdeaNRALlx32a3fJ1Dmv5rg
"""

import pandas as pd
import requests
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from bs4 import BeautifulSoup
import re
import nltk

nltk.download('punkt')
nltk.download("stopwords")

df = pd.read_csv('Input.csv')
df

li = [url for url in df['URL']]
li

import requests
import pandas as pd
from bs4 import BeautifulSoup,NavigableString
for i in range(114):
    urlid=i
    with open(str(urlid),'w')as f:
            url=li[i-1]
            r = requests.get(url, headers={"User-Agent": "XY"})
            htmlcontent=r.content
            soup=BeautifulSoup(htmlcontent,'html.parser')
            title=soup.title
            f.write(title.string)
            post_content = soup.findAll('div', attrs={'class': 'td-post-content'})
            for element in post_content:

                if not isinstance(element, NavigableString):
                    text = element.text
                    f.write(text)

l=[]
for i in range(114):
    filead=f'./{i}'
    file=open(filead)
    l.append(file.read())
df=pd.DataFrame(l,columns=['Text'])
df.head()

import pandas as pd
outputfile=pd.read_excel('Output Data Structure.xlsx')
outputfile.head()

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
l=[]
for i in range(114):

    example_sent = df['Text'][i]

    stop_words = set(stopwords.words('english'))

    word_tokens = word_tokenize(example_sent)

    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]

    filtered_sentence = []

    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)

    new_sentence=' '.join(filtered_sentence)
    l.append(new_sentence)
df=pd.DataFrame(l,columns=['Text'])

#print(word_tokens)
df.head()

import pandas as pd
pw=pd.read_csv('positive-words.txt')

new_row=pd.DataFrame({'a+':'a+'},index=[0])
pw=pd.concat([new_row,pw.loc[:].reset_index(drop=True)])
pw.columns=['0']
pw.reset_index(drop=True,inplace=True)
pw.head(100)

for i in range(114):
    ps=0
    #print(type(df['Text'][i]))
    for word in pw['0']:

        if word in df['Text'][i].lower():
            ps+=1
    outputfile.at[i,'POSITIVE SCORE']=ps

outputfile.head()

import pandas as pd
with open('negative-words.txt', 'rb') as nw:
    words = [str(w)[2:-3]for w in nw]

    nw=pd.DataFrame(words,columns=['Negative_words'])

    print(nw)

for i in range(114):
    ns=0
    #print(type(df['Text'][i]))
    for word in nw['Negative_words']:

        if word in df['Text'][i].lower():
            ns+=1
    outputfile.at[i,'NEGATIVE SCORE']=ns

outputfile.head()

for i in range(114):
    pols=0
    #print(type(df['Text'][i]))
    pos=outputfile['POSITIVE SCORE'][i]
    nes=outputfile['NEGATIVE SCORE'][i]
    pols=(pos-nes)/((pos+nes)+0.000001)


    outputfile.at[i,'POLARITY SCORE']=pols

outputfile.head()

#SUBJECTIVITY SCORE=(pos+nes)/((total no.of words)+0.000001)
for i in range(114):

    #print(type(df['Text'][i]))
    text=df['Text'][i].split()
    pos=outputfile['POSITIVE SCORE'][i]
    nes=outputfile['NEGATIVE SCORE'][i]
    nofwords=len(text)
    subs=(pos+nes)/(nofwords+0.000001)


    outputfile.at[i,'SUBJECTIVITY SCORE']=subs

outputfile.head()

from nltk.tokenize import sent_tokenize,word_tokenize
for i in range(114):

    #print(type(df['Text'][i]))
    text=df['Text'][i]
    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)
    nofwords=len(word_list)
    nofsents=len(sent_tokenize(text))
    avgsl=(nofwords)/(nofsents+0.000001)


    outputfile.at[i,'AVG SENTENCE LENGTH']=avgsl

outputfile.head()

pofcw=0
for i in range(114):
    text=df['Text'][i]
    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)
    nofwords=len(word_list)
    vowels='aeiou'
    nofcw=0
    for w in word_list:
        vc=0 #vowelcount
        for v in vowels:
            if v in w:
                vc+=1
        if vc>1:
            nofcw+=1

    pofcw=nofcw/nofwords

    outputfile.at[i,'PERCENTAGE OF COMPLEX WORDS']=pofcw

outputfile.head()

fogi=0
for i in range(114):
    text=df['Text'][i]

    #to find avgsl
    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)
    nofwords=len(word_list)
    nofsents=len(sent_tokenize(text))
    avgsl=(nofwords)/(nofsents+0.000001)

    #to find pofcw
    vowels='aeiou'
    nofcw=0
    for w in word_list:
        vc=0 #vowelcount
        for v in vowels:
            if v in w:
                vc+=1
        if vc>1:
            nofcw+=1
    pofcw=nofcw/nofwords

    fogi=0.4*(avgsl+pofcw)

    outputfile.at[i,'FOG INDEX']=fogi

outputfile.head()

for i in range(114):
    text=df['Text'][i]

    #to find avgsl
    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)
    nofwords=len(word_list)
    nofsents=len(sent_tokenize(text))
    anofwps=nofwords/nofsents

    outputfile.at[i,'AVG NUMBER OF WORDS PER SENTENCE']=anofwps

outputfile.head()

for i in range(114):
    text=df['Text'][i]

    vowels='aeiou'
    nofcw=0

    for w in word_list:
        for v in vowels:
            if v in w:
                nofcw+=1
                break


    outputfile.at[i,'COMPLEX WORD COUNT']=nofcw

outputfile.head()

#remove the punctuations
for i in range(114):
    text=df['Text'][i]

    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)
    nofwords=len(word_list)
    outputfile.at[i,'WORD COUNT']=nofwords
outputfile.head()

for i in range(114):
    text=df['Text'][i]
    vowels='aeiou'
    nofs=0
    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)

    nofwords=len(word_list)
    for l in text:
        if l in vowels:
            nofs+=1

    scpw=nofs/nofwords



    outputfile.at[i,'SYLLABLE PER WORD']=nofs

outputfile.head()

ppc=0
pp=['I','we','We','WE','my','My','MY','ours','OURS','Ours','us','Us']
for i in range(114):
    ppc=0
    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)
    for w in word_list:
        if w in pp:
            ppc+=1
    outputfile.at[i,'PERSONAL PRONOUNS']=ppc
outputfile.head()

ppc=0
pp=['I','we','We','WE','my','My','MY','ours','OURS','Ours','us','Us']
for i in range(114):
    ppc=0
    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)
    for w in word_list:
        if w in pp:
            ppc+=1
    outputfile.at[i,'PERSONAL PRONOUNS']=ppc
outputfile.head()

for i in range(114):
    text=df['Text'][i]
    #to find no. of words
    clean_text = ''.join((c for c in text if c.isalpha() or c.isspace()))
    word_list = word_tokenize(clean_text)
    nofwords=len(word_list)

    #to find sum of chars
    nofc=0
    for w in word_list:
        cpw=len(w)
        nofc+=cpw
    avgwl=nofc/nofwords


    outputfile.at[i,'AVG WORD LENGTH']=avgwl
outputfile.head()

outputfile.to_csv('mdoutputfile.csv',index=False)

